# 小智 AI (Xiaozhi ESP32) 系统架构深度解析

> **文档说明**：本文档专为毕业设计学习编写，旨在从系统架构、核心模块、数据流向及关键代码实现四个维度，深入剖析小智 AI 的运行机制。

## 1. 系统总体架构 (System Architecture)

小智 AI 是一个基于 ESP32 芯片的端云结合（Edge-Cloud）语音交互系统。其核心设计理念是将**感知（听、看）**和**执行（说、显示）**留在端侧，将**认知（理解、生成）**交给云端大模型。

### 1.1 核心分层
系统软件架构采用分层设计，自下而上分别为：

1.  **硬件抽象层 (HAL)**：
    *   `Board` 类：屏蔽不同开发板（如 S3-Box, 核心板等）的硬件差异。
    *   驱动层：I2S (音频), I2C/SPI (屏幕), GPIO (按键/LED)。
2.  **核心服务层 (Core Services)**：
    *   `AudioService`：音频采集、处理（VAD/AEC/降噪）、编码、播放。
    *   `Display`：屏幕渲染、表情管理、状态栏更新。
    *   `Network`：WiFi 连接、网络状态管理。
3.  **协议层 (Protocol Layer)**：
    *   `Protocol` (基类) -> `MqttProtocol` / `WebsocketProtocol`。
    *   负责与云端服务器建立长连接，传输音频流和控制指令（JSON）。
4.  **应用逻辑层 (Application Layer)**：
    *   `Application`：单例模式，系统的“大脑”。负责状态机管理、事件调度、模块协调。
    *   `McpServer`：基于 MCP (Model Context Protocol) 协议，实现大模型对设备的控制能力。

---

## 2. 启动流程深度分析 (Startup Flow)

系统的生命周期始于 `main/main.cc`，终于 `Application::MainEventLoop` 的无限循环。

### 2.1 启动时序
1.  **`app_main()` 入口** (`main/main.cc`)：
    *   初始化 NVS (非易失性存储)，读取 WiFi、Token 等配置。
    *   调用 `Application::GetInstance().Start()`。

2.  **`Application::Start()` 初始化** (`main/application.cc`)：
    *   **显示初始化**：`board.GetDisplay()`，屏幕亮起，显示系统信息。
    *   **音频初始化**：`audio_service_.Initialize()`，启动 I2S 总线，加载唤醒词模型。
    *   **智能系统加载**：
        *   `EventBus`：事件总线，用于模块间解耦通信。
        *   `UserProfile`：用户画像，记录用户习惯。
        *   `EmotionalMemory`：情绪记忆，模拟机器人的情感状态。
    *   **创建主任务**：`xTaskCreate(..., "main_event_loop", ...)`，启动主事件循环线程。
    *   **网络连接**：`board.StartNetwork()`，开始连接 WiFi。
    *   **OTA 检查**：`CheckNewVersion()`，检查固件更新。
    *   **协议连接**：根据配置创建 `MqttProtocol` 或 `WebsocketProtocol` 并启动连接。

---

## 3. 核心模块详解 (Core Modules)

### 3.1 大脑：Application (`main/application.cc`)
这是系统的调度中心，维护着设备的核心状态机。

*   **状态定义 (`DeviceState`)**：
    *   `kDeviceStateIdle` (待机)：等待唤醒。
    *   `kDeviceStateListening` (聆听)：正在录音并上传。
    *   `kDeviceStateSpeaking` (说话)：正在播放回复。
    *   `kDeviceStateConnecting` (连接中)：正在连接服务器。
*   **事件循环 (`MainEventLoop`)**：
    *   使用 `FreeRTOS` 的 `EventGroup` 等待事件。
    *   处理 `MAIN_EVENT_WAKE_WORD_DETECTED`：唤醒事件。
    *   处理 `MAIN_EVENT_SEND_AUDIO`：音频数据发送事件。
    *   处理 `MAIN_EVENT_SCHEDULE`：处理异步任务队列。

### 3.2 耳朵与嘴巴：AudioService (`main/audio/audio_service.cc`)
音频模块是最复杂的模块，涉及两个方向的数据流。

*   **上行链路 (Input/Mic)**：
    *   **采集**：从 I2S 读取 PCM 原始数据。
    *   **处理 (`AudioProcessor`)**：
        *   **AEC** (回声消除)：消除设备自己发出的声音，防止“自言自语”。
        *   **VAD** (语音活动检测)：判断是否有人在说话。
        *   **Wake Word** (唤醒词检测)：本地模型实时匹配“小智小智”。
    *   **编码**：使用 Opus 编码器将 PCM 压缩（24kHz -> Opus），减少网络带宽。
    *   **发送**：放入 `audio_send_queue_`，等待协议层取走发送。

*   **下行链路 (Output/Speaker)**：
    *   **接收**：协议层收到服务器的 Opus 包，放入 `audio_decode_queue_`。
    *   **解码**：Opus 解码器还原为 PCM 数据。
    *   **重采样**：调整采样率以匹配 DAC/I2S 输出要求。
    *   **播放**：写入 I2S DMA 缓冲区，驱动扬声器发声。

### 3.3 神经系统：Protocol (`main/protocols/`)
负责端云通信，定义了数据交换格式。

*   **通信载体**：
    *   **MQTT/WebSocket**：维持长连接。
    *   **UDP** (可选)：用于音频流的高速传输（MQTT 模式下）。
*   **数据包类型**：
    *   **JSON 指令**：
        *   `{"type": "tts", "state": "start"}`：开始说话。
        *   `{"type": "stt", "text": "..."}`：识别到的用户文字。
        *   `{"type": "llm", "emotion": "happy"}`：情感指令。
    *   **Audio Data**：二进制音频流。

### 3.4 肢体动作：MCP Server (`main/mcp_server.cc`)
实现了 **Model Context Protocol**，让大模型能“控制”硬件。

*   **原理**：设备向云端注册“工具(Tools)”，例如“开灯”、“调节音量”。
*   **流程**：
    1.  用户说“把音量调大”。
    2.  云端大模型识别意图，下发 MCP 调用指令 `call_tool("set_volume", {"level": 80})`。
    3.  `McpServer` 解析 JSON，找到对应的 C++ 回调函数执行。
    4.  执行结果返回给云端，大模型生成回复“好的，音量已调大”。

---

## 4. 关键交互流程全解析

### 4.1 语音交互闭环
1.  **唤醒**：
    *   用户说“小智小智”。
    *   `WakeWord` 模块匹配成功 -> `Application::OnWakeWordDetected`。
    *   状态变更为 `Listening`，播放“叮”提示音。
2.  **聆听**：
    *   `AudioService` 开始录音 -> Opus 编码 -> `Protocol::SendAudio` 上传。
    *   `VAD` 持续检测，当检测到静音超过阈值（如 800ms），触发自动停止。
3.  **思考**：
    *   发送“停止录音”信号给服务器。
    *   状态变更为 `Idle` (或 `Thinking` 动画)。
4.  **回复**：
    *   服务器下发 `tts.start` 指令 -> 状态变更为 `Speaking`。
    *   音频流到达 -> 解码播放。
    *   服务器下发 `tts.stop` 指令 -> 播放结束 -> 状态回滚至 `Idle`。

### 4.2 触摸交互
1.  **触发**：触摸屏驱动检测到点击 -> `TouchHandler::OnTouchDetected`。
2.  **处理**：
    *   构造一个特殊的 JSON 消息（如“用户摸了摸你的头”）。
    *   通过 `Protocol::SendText` 发送给云端大模型。
3.  **反馈**：
    *   大模型收到文本，认为是用户的一种交互，生成害羞或开心的回复。
    *   后续流程同语音回复。

---

## 5. 毕业设计创新点建议

如果你需要基于此项目进行毕业设计，可以关注以下创新方向：

1.  **多模态交互增强**：
    *   结合摄像头（如 ESP32-S3-Eye），增加视觉识别能力（人脸识别、物体识别），通过 MCP 传给大模型。
2.  **本地离线能力**：
    *   利用 ESP-SR 库增强离线命令词（如“打开空调”），在断网时也能控制基本家电。
3.  **个性化情感系统**：
    *   深入修改 `EmotionalMemory`，让机器人的情绪受对话内容、时间、触摸频率影响，并在屏幕上呈现更丰富的情感 UI。
4.  **物联网中控**：
    *   扩展 `McpServer`，通过 ESP32 的红外、蓝牙或 ESP-NOW 功能，控制周围的其他设备，真正实现“AI 管家”。

---

## 6. 代码阅读路线图

1.  **入门**：`main/main.cc` -> `main/application.cc` (Start 函数)。
2.  **进阶**：`main/audio/audio_service.cc` (理解音频流向)。
3.  **核心**：`main/protocols/mqtt_protocol.cc` (理解云端通信)。
4.  **扩展**：`main/mcp_server.cc` (理解如何添加新功能)。

希望这份文档能帮助你顺利完成毕业设计！
